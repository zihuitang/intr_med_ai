{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2doc(file_name):\n",
    "    \"\"\" tokenize file to doc \"\"\"\n",
    "    jieba.load_userdict(\"./data/tcm_all_dict_2.txt\")                     \n",
    "    doc = [w for x in codecs.open(file_name, 'r', 'utf-8').readlines() for w in jieba.cut(x.strip())]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word(filename):  \n",
    "    stopwords = [line.strip() for line in open(filename, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2doc_stopword(doc,file_stopword):\n",
    "    \"\"\" delete stopword from doc \"\"\"    \n",
    "    stopword_list=stop_word(file_stopword)    \n",
    "    doc_stopword=[y for y in doc if y not in stopword_list]\n",
    "    return doc_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(doc, model):\n",
    "    \"\"\"  :param model: pre-train sentence vectors model \"\"\"    \n",
    "    #start_alpha = 0.01\n",
    "    #infer_epoch = 1000\n",
    "    # text convert to sentence vector\n",
    "    #doc_vec = model.infer_vector(doc, alpha=start_alpha, steps=infer_epoch)\n",
    "    doc_vec = model.infer_vector(doc)\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(file,model):\n",
    "    file_stopword=\"./data/stopwords.txt\"\n",
    "    doc1=file2doc(file)\n",
    "    doc1stop=file2doc_stopword(doc1,file_stopword)\n",
    "    doc1_vec1=doc2vec(doc1,model)\n",
    "    return doc1_vec1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_cal(vec1, vec2):\n",
    "    \"\"\"  :return: cosin similarity rate \"\"\"\n",
    "    vec1mod = np.sqrt(vec1.dot(vec1))\n",
    "    vec2mod = np.sqrt(vec2.dot(vec2))\n",
    "    if vec2mod != 0 and vec1mod != 0:\n",
    "        sim_prob = (vec1.dot(vec2)) / (vec1mod * vec2mod)\n",
    "    else:\n",
    "        sim_prob = 0\n",
    "    return sim_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "d:\\code\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "model_path = './model/zhiwiki_news.doc2vec'\n",
    "model = gensim.models.Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Tang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.610 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102343425\n"
     ]
    }
   ],
   "source": [
    "file1 = './data/test1.txt'\n",
    "file2 = './data/test2.txt'\n",
    "\n",
    "vec1=get_vec(file1,model)\n",
    "vec2=get_vec(file2,model)\n",
    "sim_2v1=sim_cal(vec1, vec2)\n",
    "print(sim_2v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7527455\n"
     ]
    }
   ],
   "source": [
    "file1 = './data/tcm_hui_feng.txt'\n",
    "file2 = './data/tcm_han_feng.txt'\n",
    "\n",
    "vec1=get_vec(file1,model)\n",
    "vec2=get_vec(file2,model)\n",
    "sim_2v1=sim_cal(vec1, vec2)\n",
    "print(sim_2v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8775975\n"
     ]
    }
   ],
   "source": [
    "file1 = './data/tcm_hui_outline.txt'\n",
    "file2 = './data/tcm_han_outline.txt'\n",
    "\n",
    "vec1=get_vec(file1,model)\n",
    "vec2=get_vec(file2,model)\n",
    "sim_2v1=sim_cal(vec1, vec2)\n",
    "print(sim_2v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.07997514 \n",
    "0.7644119 \n",
    "0.8645462"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
