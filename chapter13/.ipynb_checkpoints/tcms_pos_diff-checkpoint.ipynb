{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"./data/data_demo_pos.xls\")\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.tcms_code\n",
    "tcme=data.iloc[:,5:]\n",
    "context=data.context_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400,) (1400,) (1400, 5)\n"
     ]
    }
   ],
   "source": [
    "print(context.shape,y.shape,tcme.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Tang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.607 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "dictfile=\"./data/tcm_all_dict_2.txt\"\n",
    "stopwords=\"./data/stopwords.txt\"\n",
    "jieba.load_userdict(dictfile)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopword(filename):  \n",
    "    stopwords = [line.strip() for line in open(filename, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(context,stopwords):\n",
    "    stopword_list=load_stopword(stopwords) \n",
    "    corpus=[]    \n",
    "    \n",
    "    for sent in context:        \n",
    "        strlist=list(jieba.cut(sent))\n",
    "        strlist=sorted(set(strlist),key=strlist.index)\n",
    "        doc_stopword=[y for y in strlist if y not in stopword_list] #drop stopword\n",
    "        corpus.append(doc_stopword)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps=get_corpus(context, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextCNN - TCM syndrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH =64\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.16 \n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1077 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "y=data.tcms_code\n",
    "all_texts=cps\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "datatxt = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(y),num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#x=datatxt\n",
    "#y1=labels\n",
    "x_train, x_test, y_train, y_test=train_test_split(datatxt,labels,test_size=0.2,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\code\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From d:\\code\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 256)           275968    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 64, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 4,865,284\n",
      "Trainable params: 4,865,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(512, 3, padding='same', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\code\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 896 samples, validate on 224 samples\n",
      "Epoch 1/8\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.8970 - acc: 0.6429 - val_loss: 0.5216 - val_acc: 0.7991\n",
      "Epoch 2/8\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.3240 - acc: 0.8828 - val_loss: 0.2934 - val_acc: 0.8661\n",
      "Epoch 3/8\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.1196 - acc: 0.9565 - val_loss: 0.2994 - val_acc: 0.8839\n",
      "Epoch 4/8\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0815 - acc: 0.9699 - val_loss: 0.3117 - val_acc: 0.8750\n",
      "Epoch 5/8\n",
      "896/896 [==============================] - 3s 3ms/step - loss: 0.0520 - acc: 0.9833 - val_loss: 0.2894 - val_acc: 0.8884\n",
      "Epoch 6/8\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0311 - acc: 0.9911 - val_loss: 0.3639 - val_acc: 0.8884\n",
      "Epoch 7/8\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.3914 - val_acc: 0.8616\n",
      "Epoch 8/8\n",
      "896/896 [==============================] - 3s 4ms/step - loss: 0.0125 - acc: 0.9955 - val_loss: 0.3905 - val_acc: 0.8929\n",
      "accuracy: 0.8857142874172755\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train, batch_size=64, epochs=4, verbose=1, validation_data=(x_test, y_test)) \n",
    "model.fit(x_train, y_train, batch_size=32, epochs=8, verbose=1, validation_split=0.2, shuffle=True) \n",
    "#score1 = model.evaluate(x_train,y_train, verbose=0)\n",
    "score2 = model.evaluate(x_test,y_test, verbose=0)\n",
    "print(\"accuracy:\",score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextCNN - TCM element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH =64\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.16 \n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb=MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ym=pd.DataFrame(tcme)\n",
    "\n",
    "all_texts=cps\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "datatxt = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = to_categorical(np.asarray(y),num_classes=None)\n",
    "print('Shape of data tensor:', datatxt.shape)\n",
    "print('Shape of label tensor:', ym.shape)\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=datatxt\n",
    "#ym=pd.DataFrame(outm)\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,ym,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(512, 3, padding='same', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "#model.add(Dense(ym.shape[1], activation='softmax')) #multiclass\n",
    "model.add(Dense(ym.shape[1], activation='sigmoid')) #multilabel\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"acc\"]) \n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"acc\"]) \n",
    "#model.fit(x_train, y_train, batch_size=64, epochs=4, verbose=1, validation_data=(x_test, y_test)) \n",
    "model.fit(x_train, y_train, batch_size=64, epochs=4, verbose=1, validation_split=0.2, shuffle=True)\n",
    "score = model.evaluate(x_test,y_test, verbose=0)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
